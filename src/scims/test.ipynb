{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97bbf361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import math\n",
    "\n",
    "files_list = []\n",
    "\n",
    "\n",
    "def temp_files_list(file_name):\n",
    "    \"\"\"\n",
    "    Adds the temporary file to the list of temporary files\n",
    "    \n",
    "    Args: file_name(str): path to the temporary file\n",
    "    \"\"\"\n",
    "    if type(file_name) in [int, list]:\n",
    "        raise TypeError\n",
    "    elif file_name in files_list:\n",
    "        pass\n",
    "    else:\n",
    "        files_list.append(file_name)\n",
    "\n",
    "\n",
    "def delete_temp_file_list():\n",
    "    \"\"\"\n",
    "    Delete the list of temporary files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for file in files_list:\n",
    "            os.unlink(file)\n",
    "    finally:\n",
    "        pass\n",
    "\n",
    "\n",
    "def check_index_files(endings, index):\n",
    "    \"\"\"\n",
    "    Check whether index files are present\n",
    "\n",
    "    Parameters:\n",
    "        endings (list): list of endings for\n",
    "        index (str): Prefix of the index\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        if \"/\" in index:\n",
    "            path = \"/\".join(index.split(\"/\")[:-1])\n",
    "            index = index.split(\"/\")[-1]\n",
    "            subprocess.run([\"ls\", path], stdout=f)\n",
    "        else:\n",
    "            subprocess.run([\"ls\"], stdout=f)\n",
    "    bool_count = 0\n",
    "    with open(f.name) as fn:\n",
    "        lines = fn.readlines()\n",
    "        for pattern in endings:\n",
    "            for i in range(len(lines)):\n",
    "                if lines[i].strip(\"\\n\").endswith(pattern) and index in lines[i]:\n",
    "                    bool_count += 1\n",
    "    os.unlink(f.name)\n",
    "    if bool_count == len(endings):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_bwa_index(index):\n",
    "    \"\"\"\n",
    "    Verify index files are present for BWA\n",
    "\n",
    "    Parameters:\n",
    "        index (str): Prefix for BWA index\n",
    "    \"\"\"\n",
    "    if check_index_files([\"amb\", \"ann\", \"bwt\", \"pac\", \"sa\"], index):\n",
    "        return True\n",
    "    else:\n",
    "        raise Exception(\"BWA index files can not be found\")\n",
    "\n",
    "\n",
    "def align_with_bwa(index, forward_reads, reverse_reads, threads):\n",
    "    \"\"\"\n",
    "        Align paired-end reads with BWA\n",
    "\n",
    "        Parameters:\n",
    "            index (str): prefix for BWA index\n",
    "            forward_reads (str): path to file with forward FASTQ reads\n",
    "            reverse_reads (str): path to file with reverse FASTQ reads\n",
    "            threads (int): number of threads to use\n",
    "\n",
    "        Returns:\n",
    "            f.name (str): Path to output SAM file\n",
    "    \"\"\"\n",
    "    check_bwa_index(index)\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        subprocess.run([\"bwa\", \"mem\", \"-t\", threads, index, forward_reads, reverse_reads],\n",
    "                       stdout=f, stderr=subprocess.DEVNULL)\n",
    "        temp_files_list(f.name)\n",
    "        return f.name\n",
    "\n",
    "def verify_sam_file(sam_file):\n",
    "    \"\"\"\n",
    "    Verify that the SAM file is valid\n",
    "\n",
    "    Parameters:\n",
    "        sam_file (str): Path to SAM file\n",
    "\n",
    "    Returns:\n",
    "        (bool): Whether the SAM file is valid or not\n",
    "    \"\"\"\n",
    "    with open(sam_file) as fn:\n",
    "        flag = False\n",
    "        for line in fn:\n",
    "            if line.startswith(\"@\"):\n",
    "                continue\n",
    "            else:\n",
    "                flag = True\n",
    "                if len(line.split(\"\\t\")) >= 11:\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "        return flag\n",
    "\n",
    "def get_human_sequences(input_sam):\n",
    "    \"\"\"\n",
    "    Get the sequences from BWA alignment output\n",
    "    that map to the human genome\n",
    "\n",
    "    Parameters:\n",
    "        input_sam (str): Path to input SAM file\n",
    "\n",
    "    Returns:\n",
    "        f.name (str): Path to output BAM file\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        if verify_sam_file(input_sam):\n",
    "            subprocess.run(\n",
    "                [\"samtools\", \"view\", \"-F\", \"4\", \"-F\", \"8\", \"-b\", input_sam],\n",
    "                stdout=f,\n",
    "                stderr=subprocess.DEVNULL,\n",
    "            )\n",
    "            temp_files_list(f.name)\n",
    "            return f.name\n",
    "        else:\n",
    "            raise Exception(\"Not a valid SAM file\")\n",
    "\n",
    "\n",
    "def get_sex_sequences(\n",
    "        input_bam, homogamete, heterogamete, is_filter=True, mapq_min=50, tlen=75\n",
    "):\n",
    "    \"\"\"\n",
    "    Get the sequences that map to the sex chromosomes\n",
    "\n",
    "    Parameters:\n",
    "        input_bam (str): Path to input BAM file\n",
    "        homogamete (str): FASTA ID for homogametic element\n",
    "        heterogamete (str): FASTA ID for heterogametic element\n",
    "        is_filter (bool): Filter reads based on quality thresholds\n",
    "        mapq_min (int): Minimum mapq score\n",
    "        tlen (int): Template length\n",
    "\n",
    "    Returns:\n",
    "        g.name (str): Path to SAM file with reads that mapped to sex chromosomes\n",
    "\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        subprocess.run([\"samtools\", \"view\", \"-F\", \"2048\", \"-F\", \"256\", input_bam], stdout=f)\n",
    "        temp_files_list(f.name)\n",
    "\n",
    "    if is_filter:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as g:\n",
    "            for line in open(f.name):\n",
    "                rec = ReadSamLine(line)\n",
    "                rec.getFeatures()\n",
    "                if rec.mapq < mapq_min:\n",
    "                    if rec.rnam == homogamete or rec.rnam == heterogamete:\n",
    "                        if rec.tlen > tlen or rec.tlen < (tlen * -1):\n",
    "                            g.write(line.encode())\n",
    "            temp_files_list(g.name)\n",
    "            return g.name\n",
    "    else:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as g:\n",
    "            for line in open(f.name):\n",
    "                rec = ReadSamLine(line)\n",
    "                rec.getFeatures()\n",
    "                if rec.rnam == homogamete or rec.rnam == heterogamete:\n",
    "                    if rec.tlen > tlen or rec.tlen < (tlen * -1):\n",
    "                        g.write(line.encode())\n",
    "            temp_files_list(g.name)\n",
    "            return g.name\n",
    "\n",
    "\n",
    "def get_unique_ids_in_sam(input_sam):\n",
    "    \"\"\"\n",
    "    Get the unique query names from the SAM file\n",
    "\n",
    "    Parameters:\n",
    "        input_sam (str): Path to input SAM file\n",
    "\n",
    "    returns:\n",
    "        (list): Set of unique FASTA IDs\n",
    "    \"\"\"\n",
    "    read_ids = []\n",
    "    with open(input_sam) as fn:\n",
    "        for rec in fn:\n",
    "            line = ReadSamLine(rec)\n",
    "            line.getFeatures()\n",
    "            read_ids.append(line.query)\n",
    "    return set(read_ids)\n",
    "\n",
    "\n",
    "def get_fastq_reads_in_sam(read_ids, forward_reads, reverse_reads):\n",
    "    \"\"\"\n",
    "    Get FASTQ reads for sequences in SAM file\n",
    "\n",
    "    Parameters:\n",
    "        read_ids (list): List of FASTA IDs\n",
    "        forward_reads (str): Path to forward FASTQ reads\n",
    "        reverse_reads (str): Path to reverse FASTQ reads\n",
    "\n",
    "    Returns:\n",
    "        out_fastq1.name (str): Path to output forward FASTQ file\n",
    "        out_fastq2.name (str): Path to output reverse FASTQ file\n",
    "    \"\"\"\n",
    "\n",
    "    # Open output files\n",
    "    out_fastq1, out_fastq2 = (\n",
    "        tempfile.NamedTemporaryFile(delete=False),\n",
    "        tempfile.NamedTemporaryFile(delete=False),\n",
    "    )\n",
    "\n",
    "    # Get whether the file is FASTA or FASTQ\n",
    "    file_type = fastaOrFastq(forward_reads)\n",
    "\n",
    "    # Open the input read files and get those FASTQ sequences from the SAM file\n",
    "    if isFileGzip(forward_reads):\n",
    "        forward_reads = gzip.open(forward_reads, \"rt\")\n",
    "        reverse_reads = gzip.open(reverse_reads, \"rt\")\n",
    "\n",
    "    for rec1 in SeqIO.parse(forward_reads, file_type):\n",
    "        if str(rec1.id)[:-2] in read_ids:\n",
    "            out_fastq1.write((\"@\" + str(rec1.description)[:-2] + \"\\n\").encode())\n",
    "            out_fastq1.write((str(rec1.seq) + \"\\n\").encode())\n",
    "            out_fastq1.write(\"+\\n\".encode())\n",
    "            quality = \"\"\n",
    "            for qual in rec1.letter_annotations[\"phred_quality\"]:\n",
    "                quality = quality + str(chr(qual + 33))\n",
    "            out_fastq1.write((str(quality) + \"\\n\").encode())\n",
    "\n",
    "    for rec2 in SeqIO.parse(reverse_reads, file_type):\n",
    "        if str(rec2.id)[:-2] in read_ids:\n",
    "            out_fastq2.write((\"@\" + str(rec2.description)[:-2] + \"\\n\").encode())\n",
    "            out_fastq2.write((str(rec2.seq) + \"\\n\").encode())\n",
    "            out_fastq2.write(\"+\\n\".encode())\n",
    "            quality = \"\"\n",
    "            for qual in rec2.letter_annotations[\"phred_quality\"]:\n",
    "                quality = quality + str(chr(qual + 33))\n",
    "            out_fastq2.write((str(quality) + \"\\n\").encode())\n",
    "\n",
    "    out_fastq1.close()\n",
    "    out_fastq2.close()\n",
    "    temp_files_list(out_fastq1.name)\n",
    "    temp_files_list(out_fastq2.name)\n",
    "    return out_fastq1.name, out_fastq2.name\n",
    "\n",
    "\n",
    "def merge_with_flash(pair1, pair2, mismatch_ratio=0.1, max_overlap=150, min_overlap=40):\n",
    "    \"\"\"\n",
    "    Merge paired-end reads with FLASH\n",
    "\n",
    "    Parameters:\n",
    "        pair1 (str): Path to forward FASTQ file\n",
    "        pair2 (str): Path to reverse FASTQ file\n",
    "        mismatch_ratio (float): Maximum mismatch ratio\n",
    "        max_overlap (int): Maximum overlap for read merger\n",
    "        min_overlap (int): Minimum overlap for read merger\n",
    "\n",
    "    Returns:\n",
    "        merged (str): Path to merged FASTQ file\n",
    "        unmerged1 (str): Path to forward, unmerged FASTQ file\n",
    "        unmerged2 (str): Path to reverse, unmerged FASTQ file\n",
    "    \"\"\"\n",
    "    tempdir = tempfile.TemporaryDirectory()\n",
    "    subprocess.run(\n",
    "        [\n",
    "            \"flash\",\n",
    "            \"-x\",\n",
    "            str(mismatch_ratio),\n",
    "            \"-M\",\n",
    "            str(max_overlap),\n",
    "            \"-m\",\n",
    "            str(min_overlap),\n",
    "            pair1,\n",
    "            pair2,\n",
    "            \"-d\",\n",
    "            tempdir.name,\n",
    "        ],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "    )\n",
    "    merged = convert_to_temp(tempdir.name + \"/out.extendedFrags.fastq\")\n",
    "    unmerged1 = convert_to_temp(tempdir.name + \"/out.notCombined_1.fastq\")\n",
    "    unmerged2 = convert_to_temp(tempdir.name + \"/out.notCombined_2.fastq\")\n",
    "    return merged, unmerged1, unmerged2\n",
    "\n",
    "\n",
    "def convert_to_temp(old_file):\n",
    "    \"\"\"\n",
    "    Convert a file into a temporary file\n",
    "\n",
    "    Parameters:\n",
    "        old_file (str): Path to input file\n",
    "\n",
    "    Returns:\n",
    "        f.name (str): Path to output temporary file\n",
    "    \"\"\"\n",
    "    with open(old_file) as fn:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "            for line in fn:\n",
    "                f.write(line.encode())\n",
    "            return f.name\n",
    "\n",
    "\n",
    "def align_merged_with_bowtie2(index, merged, threads=1, no_of_align_per_read=50):\n",
    "    \"\"\"\n",
    "    Align merged reads with bowtie2\n",
    "\n",
    "    Parameters:\n",
    "        index (str): Prefix for Bowite2 index\n",
    "        merged (str): Path to FLASH merged, FASTQ reads\n",
    "        threads (int): Number of threads to use\n",
    "        no_of_align_per_read (int): Maximum number of alignments per-read\n",
    "\n",
    "    Returns:\n",
    "        f.name (str): Path to output SAM file\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"bowtie2\",\n",
    "                \"-p\",\n",
    "                str(threads),\n",
    "                \"-k\",\n",
    "                str(no_of_align_per_read),\n",
    "                \"--local\",\n",
    "                \"-x\",\n",
    "                index,\n",
    "                \"-U\",\n",
    "                merged,\n",
    "            ],\n",
    "            stdout=f,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "        temp_files_list(f.name)\n",
    "        return f.name\n",
    "\n",
    "\n",
    "def paired_reads_with_bowtie2(\n",
    "        index, forward_reads, reverse_reads, threads=1, no_of_align_per_read=50\n",
    "):\n",
    "    \"\"\"\n",
    "    Align paired reads with bowtie2\n",
    "\n",
    "    Parameters:\n",
    "        index (str): Prefix for Bowite2 index\n",
    "        forward_reads (str): Path to forward FASTQ reads\n",
    "        reverse_reads (str): Path to reverse FASTq reads\n",
    "        threads (int): Number of threads to use\n",
    "        no_of_align_per_read (int): Maximum number of alignments per-read\n",
    "\n",
    "    Returns:\n",
    "        f.name (str): Path to output SAM file\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        subprocess.run(\n",
    "            [\n",
    "                \"bowtie2\",\n",
    "                \"-p\",\n",
    "                str(threads),\n",
    "                \"-k\",\n",
    "                str(no_of_align_per_read),\n",
    "                \"--local\",\n",
    "                \"-x\",\n",
    "                index,\n",
    "                \"-1\",\n",
    "                forward_reads,\n",
    "                \"-2\",\n",
    "                reverse_reads,\n",
    "            ],\n",
    "            stdout=f,\n",
    "            stderr=subprocess.DEVNULL,\n",
    "        )\n",
    "        return f.name\n",
    "\n",
    "\n",
    "def get_read_data_from_merged_sam(sam_file):\n",
    "    \"\"\"\n",
    "    Get data from the SAM file of merged reads\n",
    "\n",
    "    Parameters:\n",
    "        sam_file (str): Path to input SAM file\n",
    "\n",
    "    Returns:\n",
    "        fastq_data (list): A list of dictionaries where each element\n",
    "                            corresponds to each line in the SAM file\n",
    "    \"\"\"\n",
    "    fastq_data = []\n",
    "    for rec in ParseSam(sam_file):\n",
    "        if rec.cigar != \"*\":\n",
    "            num_matches = extractFromCigar(\"M\", rec.cigar)\n",
    "            num_deletions = extractFromCigar(\"D\", rec.cigar)\n",
    "\n",
    "            if \"UNMAP\" not in decompose_sam_flag(rec.flag) and \"MUNMAP\" not in decompose_sam_flag(rec.flag):\n",
    "                read_data = {\n",
    "                    \"read_name\": rec.query,\n",
    "                    \"chrom\": rec.rnam,\n",
    "                    \"alignment_score\": rec.align_score,\n",
    "                    \"num_mismatches\": rec.mismatches,\n",
    "                    \"insert_size\": None,\n",
    "                    \"pos_1\": rec.pos,\n",
    "                    \"pos_2\": -1,\n",
    "                    \"num_matches\": num_matches,\n",
    "                    \"num_deletions\": num_deletions,\n",
    "                }\n",
    "\n",
    "                fastq_data.append(read_data)\n",
    "    return fastq_data\n",
    "\n",
    "\n",
    "def get_read_data_from_unmerged_sam(unmerged_sam):\n",
    "    \"\"\"\n",
    "    Get data from SAM file of unmerged reads\n",
    "\n",
    "    Parameters:\n",
    "        unmerged_sam (str): Path to input SAM file\n",
    "\n",
    "    Returns:\n",
    "        fastq_data (list): List of dictionaries where each element\n",
    "                            in the list corresponds to a line in\n",
    "                            the SAM file\n",
    "    \"\"\"\n",
    "    fastq_data = []\n",
    "    for rec in ParseSam(unmerged_sam):\n",
    "        if rec.cigar != \"*\":\n",
    "            num_matches = extractFromCigar(\"M\", rec.cigar)\n",
    "            num_deletions = extractFromCigar(\"D\", rec.cigar)\n",
    "            read_data = {\n",
    "                \"read_name\": rec.query,\n",
    "                \"sam_flag\": rec.flag,\n",
    "                \"chrom\": rec.rnam,\n",
    "                \"alignment_score\": rec.align_score,\n",
    "                \"num_mismatches\": rec.mismatches,\n",
    "                \"mate_start\": rec.pnext,\n",
    "                \"insert_size\": rec.tlen,\n",
    "                \"pos\": rec.pos,\n",
    "                \"num_matches\": num_matches,\n",
    "                \"num_deletions\": num_deletions,\n",
    "            }\n",
    "            fastq_data.append(read_data)\n",
    "    return fastq_data\n",
    "\n",
    "\n",
    "###########################################\n",
    "# Filter reads based on mapping info\n",
    "###########################################\n",
    "\n",
    "def not_close_to_max(pandas_df, diff_from_max_percent):\n",
    "    \"\"\"\n",
    "    Filter sequences that are not within a certain\n",
    "    percentage to the quality of the highest scoring\n",
    "    alignment for that read\n",
    "\n",
    "    Parameters:\n",
    "        pandas_df (pandas data frame):\n",
    "        diff_from_max_percent (float): Maximum difference in alignment quality between\n",
    "                                    a given read and the best alignment for that read\n",
    "    Return:\n",
    "        Filtered data frame\n",
    "    \"\"\"\n",
    "\n",
    "    pandas_df[\"max_score_read_name\"] = pandas_df.groupby(\"read_name\")[\"alignment_score\"].transform(max)\n",
    "    pandas_df[\"diff_percent_max_round\"] = np.round(\n",
    "        pandas_df[\"max_score_read_name\"] * diff_from_max_percent\n",
    "    )\n",
    "    pandas_df[\"score_max_diff\"] = pandas_df[\"max_score_read_name\"] - pandas_df[\"alignment_score\"]\n",
    "    return pandas_df[pandas_df[\"score_max_diff\"] <= pandas_df[\"diff_percent_max_round\"]]\n",
    "\n",
    "\n",
    "def min_num_of_matches(pandas_df, min_match):\n",
    "    \"\"\"\n",
    "    Filter out reads with less than a certain number of reads matching\n",
    "\n",
    "    Parameters:\n",
    "        pandas_df (pandas data frame):\n",
    "        min_match (int)\n",
    "\n",
    "    Returns:\n",
    "        (pandas data frame): Filtered data frame\n",
    "    \"\"\"\n",
    "    return pandas_df[pandas_df[\"num_matches\"] > min_match]\n",
    "\n",
    "\n",
    "def too_many_mismatch(pandas_df, mismatch_ratio):\n",
    "    \"\"\"\n",
    "    Filter out alignments that have too many mismatches\n",
    "\n",
    "    Parameters:\n",
    "        pandas_df (pandas data frame): Pandas data frame with read information\n",
    "        mismatch_ratio (float): Ratio of mismatches to matches\n",
    "\n",
    "    Returns:\n",
    "        (pandas data frame): Filtered pandas data frame\n",
    "    \"\"\"\n",
    "    pandas_df[\"mismatch_ratio\"] = pandas_df[\"num_mismatches\"].astype(float) / pandas_df[\n",
    "        \"num_matches\"\n",
    "    ].astype(float)\n",
    "    return pandas_df[pandas_df[\"mismatch_ratio\"] < mismatch_ratio]\n",
    "\n",
    "\n",
    "def too_many_deletions(pandas_df, num_deletions):\n",
    "    \"\"\"\n",
    "    Filter out alignments that have too many deletions\n",
    "    \"\"\"\n",
    "    return pandas_df[pandas_df[\"num_deletions\"] < num_deletions]\n",
    "\n",
    "\n",
    "def map_to_one_chrom(pandas_df):\n",
    "    \"\"\"\n",
    "    Filter out alignments that only map to one chromosome\n",
    "    \"\"\"\n",
    "    mapper = pandas_df.groupby(\"read_name\")[\"chrom\"].nunique().to_dict()\n",
    "    pandas_df[\"unique_chroms\"] = pandas_df[\"read_name\"].map(mapper)\n",
    "    return pandas_df[pandas_df[\"unique_chroms\"] == 1]\n",
    "\n",
    "# ----------------------------------------------\n",
    "\n",
    "\n",
    "def filter_merged_sam(\n",
    "        merged_sam,\n",
    "        diff_from_max_percent=0.025,\n",
    "        min_match=75,\n",
    "        mismatch_ratio=0.025,\n",
    "        num_deletions=3,\n",
    "):\n",
    "    \"\"\"\n",
    "    Filter reads from the SAM file of merged reads based on certain filtering stats\n",
    "\n",
    "    Parameters:\n",
    "         merged_sam (str): Path to SAM file of merged reads\n",
    "         diff_from_max_percent (float): Maximum difference in alignment quality between\n",
    "                                    a given read and the best alignment for that read\n",
    "        min_match (int): Minimum number of bases matching between the query and reference\n",
    "        mismatch_ratio (float): Maximum ratio of matches to mismatches between query\n",
    "                                and reference\n",
    "        num_deletions (float): Maximum number of deletions\n",
    "\n",
    "    Returns:\n",
    "        df (pandas data frame): Data frame with info for sequences that survived read rescue\n",
    "    \"\"\"\n",
    "    fastq_data = get_read_data_from_merged_sam(merged_sam)\n",
    "\n",
    "    if fastq_data:\n",
    "        # Transform the fastq_data into a pandas dataframe\n",
    "        df = pd.DataFrame(fastq_data)\n",
    "\n",
    "        df = not_close_to_max(df, diff_from_max_percent)\n",
    "\n",
    "        # Filter out reads with less than a certain number of bases matching.\n",
    "        df = min_num_of_matches(df, min_match)\n",
    "\n",
    "        df = too_many_mismatch(df, mismatch_ratio)\n",
    "\n",
    "        # Filter out reads with too many deletions.\n",
    "        df = too_many_deletions(df, num_deletions)\n",
    "\n",
    "        # Finally keep reads that have only one annotated chromosome.\n",
    "        df = map_to_one_chrom(df)\n",
    "        return df\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def filter_unmerged_sam(\n",
    "        unmerged_sam, diff_from_max_percent=0.025, min_match=100, num_deletions=3,\n",
    "        mismatch_ratio=0.025\n",
    "):\n",
    "    \"\"\"\"\n",
    "    Filter SAM file of unmerged reads\n",
    "\n",
    "    Parameters:\n",
    "        unmerged_sam (str): Path to SAM file of unmerged reads\n",
    "        diff_from_max_percent (float): Maximum difference in alignment quality between\n",
    "                                    a given read and the best alignment for that read\n",
    "        min_match (int): Minimum number of bases matching between the query and reference\n",
    "        num_deletions (float): Maximum number of deletions\n",
    "        mismatch_ratio (float): Ratio of mismatches to matches\n",
    "\n",
    "    Returns:\n",
    "        df_pair (pandas data frame): Data frame with data for reads that survived\n",
    "                                    read rescue\n",
    "\n",
    "    \"\"\"\n",
    "    read_pair = []\n",
    "    fastq_data = []\n",
    "    read_dict = {}\n",
    "    unmerged_read_data = get_read_data_from_unmerged_sam(unmerged_sam)\n",
    "\n",
    "    for read in unmerged_read_data:\n",
    "        if read[\"read_name\"] in read_dict:\n",
    "            if set([\"READ2\",\"PROPER_PAIR\"]).issubset(set(decompose_sam_flag(read[\"sam_flag\"]))):\n",
    "                read_dict[read[\"read_name\"]].append(read)\n",
    "        elif set([\"READ1\",\"PROPER_PAIR\"]).issubset(set(decompose_sam_flag(read[\"sam_flag\"]))):\n",
    "            read_dict[read[\"read_name\"]] = [read]\n",
    "\n",
    "    for read_pair in read_dict.items():\n",
    "        total_num_matches = read_pair[1][0][\"num_matches\"] + read_pair[1][1][\"num_matches\"]\n",
    "        total_num_mismatches = (read_pair[1][0][\"num_mismatches\"] + read_pair[1][1][\"num_mismatches\"])\n",
    "        total_num_deletions = (read_pair[1][0][\"num_deletions\"] + read_pair[1][1][\"num_deletions\"])\n",
    "        total_alignment_score = (read_pair[1][0][\"alignment_score\"] + read_pair[1][1][\"alignment_score\"])\n",
    "        fastq_data.append(\n",
    "                        {\n",
    "                            \"read_name\": read_pair[0],\n",
    "                            \"num_mismatches\": total_num_mismatches,\n",
    "                            \"chrom\": read_pair[1][0][\"chrom\"],\n",
    "                            \"num_matches\": total_num_matches,\n",
    "                            \"num_deletions\": total_num_deletions,\n",
    "                            \"insert_size\": abs(read_pair[1][0][\"insert_size\"]),\n",
    "                            \"alignment_score\": total_alignment_score,\n",
    "                            \"pos_1\": read_pair[1][0][\"pos\"],\n",
    "                            \"pos_2\": read_pair[1][1][\"pos\"],\n",
    "                        }\n",
    "                    )\n",
    "    df_pair = pd.DataFrame(fastq_data)\n",
    "\n",
    "    # Filter reads with big insert size (gt 999)\n",
    "    df_pair = df_pair[df_pair[\"insert_size\"] < 1000]\n",
    "\n",
    "    # Filter out reads less than 2.5% max score.\n",
    "    df_pair = not_close_to_max(df_pair, diff_from_max_percent)\n",
    "\n",
    "    # Filter out reads with less than 100 bp matching.\n",
    "    df_pair = min_num_of_matches(df_pair, min_match)\n",
    "\n",
    "    # Filter out reads with too many mismatches (more than 1 mismatch every 40 bp).\n",
    "    df_pair = too_many_mismatch(df_pair, mismatch_ratio)\n",
    "\n",
    "    # Filter out reads with too many deletions.\n",
    "    df_pair = df_pair[df_pair[\"num_deletions\"] <= num_deletions]\n",
    "\n",
    "    # Finally keep reads that have only one annotated chromosome.\n",
    "    df_pair = map_to_one_chrom(df_pair)\n",
    "    print(df_pair['chrom'].value_counts())\n",
    "    return df_pair\n",
    "\n",
    "\n",
    "def combine_df(df1, df2):\n",
    "    \"\"\"\n",
    "    Combine 2 pandas data frames\n",
    "\n",
    "    Parameters:\n",
    "        df1 (pandas data frame): Data frame 1\n",
    "        df2 (pandas data frame): Data frame 2\n",
    "\n",
    "    Returns:\n",
    "        (pandas df): Concatenated data frame\n",
    "\n",
    "    \"\"\"\n",
    "    return pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "def bam2sam(bam_file):\n",
    "    \"\"\"\n",
    "    Convert BAM files to SAM files\n",
    "\n",
    "    Parameters:\n",
    "        bam_file (str): path to BAM file\n",
    "\n",
    "    Returns:\n",
    "        f.name (str): path to SAM file\n",
    "    \"\"\"\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as f:\n",
    "        subprocess.run([\"samtools\", \"view\", bam_file], stdout=f)\n",
    "        temp_files_list(f.name)\n",
    "        return f.name\n",
    "\n",
    "\n",
    "def get_start_and_stop(in_sam, min_mapq):\n",
    "    \"\"\"\n",
    "    Get the start and stop positions for reads in a SAM file\n",
    "\n",
    "    Parameters:\n",
    "        in_sam (str): Path to input SAM file\n",
    "        min_mapq (int): Minimum mapq score\n",
    "\n",
    "    Returns:\n",
    "        seen_starts (list):\n",
    "    \"\"\"\n",
    "    seen_starts, seen_ends = set(), set()\n",
    "    for rec in ParseSam(in_sam):\n",
    "        if \"SECONDARY\" in decompose_sam_flag(rec.flag) or \"SUPPLEMENTARY\" in decompose_sam_flag(rec.flag):\n",
    "            continue\n",
    "        if \"READ1\" in decompose_sam_flag(rec.flag) and rec.tlen >= 0:\n",
    "            if rec.mapq >= min_mapq:\n",
    "                seen_starts.add(\"{}:{}\".format(rec.rnam, rec.pos))\n",
    "                seen_ends.add(\"{}:{}\".format(rec.rnam, rec.pos + rec.tlen - 1))\n",
    "    return seen_starts, seen_ends\n",
    "\n",
    "\n",
    "def read_rescue_update(df, in_sam, min_mapq):\n",
    "    \"\"\"\n",
    "    Preform 'read rescue update'\n",
    "\n",
    "    Parameters:\n",
    "        df (pandas data frame): data frame from combine_df()\n",
    "        in_sam (str): path to input BAM file\n",
    "        min_mapq (int): Minimum mapq score\n",
    "\n",
    "    Returns:\n",
    "        out_sam.name (str): Path to output SAM\n",
    "    \"\"\"\n",
    "    ok_reads = set()\n",
    "    with tempfile.NamedTemporaryFile(delete=False) as out_sam:\n",
    "        starts_and_stops = get_start_and_stop(in_sam, min_mapq)\n",
    "        seen_starts, seen_ends = starts_and_stops[0], starts_and_stops[1]\n",
    "        group_df_dict = {g: gdf for g, gdf in df.groupby(\"read_name\")}\n",
    "\n",
    "        for line in open(in_sam, \"r\"):\n",
    "            if line.startswith(\"@\"):\n",
    "                out_sam.write(line.encode())\n",
    "                continue\n",
    "            sam_fields = line.strip().split(\"\\t\")\n",
    "            header = sam_fields[0]\n",
    "            chrom = sam_fields[2]\n",
    "            mapq_score = int(sam_fields[4])\n",
    "            # Check if read has been added already. if it has write and\n",
    "            if header in ok_reads:\n",
    "                out_sam.write(line.encode())\n",
    "                continue\n",
    "            elif mapq_score >= min_mapq:\n",
    "                out_sam.write(line.encode())\n",
    "                ok_reads.add(header)\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    sub_df = group_df_dict[header]\n",
    "                except:\n",
    "                    continue\n",
    "                do_not_add = False\n",
    "                if not sub_df.empty:\n",
    "                    if not sub_df[\"chrom\"].iloc[0] == chrom:\n",
    "                        continue\n",
    "                    to_add_start = []\n",
    "                    to_add_end = []\n",
    "                    for i, row in sub_df.iterrows():\n",
    "                        coord_start = \"{}:{}\".format(row[\"chrom\"], row[\"pos_1\"])\n",
    "                        if int(row[\"pos_2\"]) != -1:\n",
    "                            coord_end = \"{}:{}\".format(row[\"chrom\"], row[\"pos_2\"])\n",
    "                        else:\n",
    "                            coord_end = False\n",
    "                        if coord_start in seen_starts:\n",
    "                            do_not_add = True\n",
    "                        else:\n",
    "                            if coord_end:\n",
    "                                if coord_end in seen_ends:\n",
    "                                    do_not_add = True\n",
    "                        if not do_not_add:\n",
    "                            to_add_start.append(coord_start)\n",
    "                            if coord_end:\n",
    "                                to_add_end.append(coord_end)\n",
    "                else:\n",
    "                    do_not_add = True\n",
    "                if not do_not_add:\n",
    "                    for coord in to_add_start:\n",
    "                        seen_starts.add(coord)\n",
    "                    for coord in to_add_end:\n",
    "                        seen_ends.add(coord)\n",
    "                    ok_reads.add(header)\n",
    "                    out_sam.write(line.encode())\n",
    "        return out_sam.name\n",
    "\n",
    "\n",
    "def count_chrom(sam, hom, het):\n",
    "    \"\"\"\n",
    "    Count the number of reads that map to the homogametic and\n",
    "    heterogametic elements\n",
    "\n",
    "    Parameters:\n",
    "        sam (str): path to SAM File\n",
    "        hom (str): FASTA identifier for homogametic element\n",
    "        het (str): FASTA identifier for heterogametic element\n",
    "\n",
    "    Returns:\n",
    "        hom_counts (int): Count of reads that mapped to homogametic element\n",
    "        het_counts (int): Count of reads that mapped to heterogametic element\n",
    "    \"\"\"\n",
    "    het_counts, hom_counts = 0, 0\n",
    "    for rec in ParseSam(sam):\n",
    "        if rec.rnam == hom:\n",
    "            hom_counts += 1\n",
    "        elif rec.rnam == het:\n",
    "            het_counts += 1\n",
    "    return hom_counts, het_counts\n",
    "\n",
    "\n",
    "def calculate_stats(homogametic_counts, heterogametic_counts):\n",
    "    \"\"\"\n",
    "    Calculate statistics for sequence counts\n",
    "\n",
    "    Parameters:\n",
    "        homogametic_counts (int): Number of reads that mapped to homogametic element\n",
    "        heterogametic_counts (int): Number of reads that mapped to heterogametic element\n",
    "\n",
    "    Returns:\n",
    "        round(prop, 3) (float): Proportion of heterogametic to homogametic reads\n",
    "                                rounded to 3 decimal places\n",
    "        round(ci, 3) (float): Confidence interval for read proportions\n",
    "    \"\"\"\n",
    "    prop = heterogametic_counts / (homogametic_counts + heterogametic_counts)\n",
    "    ci = 1.96 * math.sqrt(\n",
    "        (prop * (1 - prop)) / (heterogametic_counts + homogametic_counts)\n",
    "    )\n",
    "    return round(prop, 3), round(ci, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c33ed47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d838e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import subprocess\n",
    "import tempfile\n",
    "\n",
    "class ParseSam:\n",
    "    \"\"\"Parse a SAM file and return a SAM record\n",
    "    for each sequence in the SAM file\n",
    "\n",
    "    usage example:\n",
    "    for sam_record in ParseSam(\"sam_file.sam\"):\n",
    "            print(rec.query)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samFile):\n",
    "        self.samFile = samFile\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.fn = open(self.samFile)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        line = self.fn.__next__()\n",
    "        while line.startswith(\"@\"):\n",
    "            line = self.fn.__next__()\n",
    "        samData = ReadSamLine(line)\n",
    "        samData.getFeatures()\n",
    "        return samData\n",
    "\n",
    "\n",
    "class ReadSamLine:\n",
    "    \"\"\"\n",
    "    Read a line from a SAM file and get each of the fields.\n",
    "    Naming conventions are based on the SAM format\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, samLine):\n",
    "        self.samLine = samLine\n",
    "\n",
    "    def getFeatures(self):\n",
    "        \"\"\"\n",
    "        Takes a line from a SAM file as\n",
    "        a string collects information\n",
    "        from the fields\n",
    "        \"\"\"\n",
    "\n",
    "        if not self.samLine.startswith(\"@\"):\n",
    "            samFields = self.samLine.strip(\"\\n\").split(\"\\t\")\n",
    "            self.query = samFields[0]\n",
    "            self.flag = int(samFields[1])\n",
    "            self.rnam = samFields[2]\n",
    "            self.pos = int(samFields[3])\n",
    "            self.mapq = int(samFields[4])\n",
    "            self.cigar = samFields[5]\n",
    "            self.rnext = samFields[6]\n",
    "            self.pnext = samFields[7]\n",
    "            self.tlen = int(samFields[8])\n",
    "            self.seq = samFields[9]\n",
    "            self.qual = samFields[10]\n",
    "            self.align_score = None\n",
    "            self.align_score_best = None\n",
    "            self.align_score_mate = None\n",
    "            self.num_ambig = None\n",
    "            self.mismatches = None\n",
    "            self.gap_opens = None\n",
    "            self.gap_ext = None\n",
    "            self.edit_distance = None\n",
    "            self.why_filtered = None\n",
    "            self.yt = None\n",
    "            self.mismatch_ref_bases = None\n",
    "\n",
    "            if len(samFields) >= 10:\n",
    "                # If the file is a bowtie file, get the bowtie-specific\n",
    "                # fields in the SAM file\n",
    "                for field in samFields[10:]:\n",
    "                    if \"AS:i\" in field:\n",
    "                        self.align_score = int(field.strip(\"AS:i\"))\n",
    "                    elif \"XS:i\" in field:\n",
    "                        self.align_score_best = int(field.strip(\"XS:i\"))\n",
    "                    elif \"YS:i\" in field:\n",
    "                        self.align_score_mate = int(field.strip(\"YS:i\"))\n",
    "                    elif \"XN:i\" in field:\n",
    "                        self.num_ambig = int(field.strip(\"XN:i\"))\n",
    "                    elif \"XM:i\" in field:\n",
    "                        self.mismatches = int(field.strip(\"XM:i\"))\n",
    "                    elif \"XO:i\" in field:\n",
    "                        self.gap_opens = int(field.strip(\"XO:i\"))\n",
    "                    elif \"XG:i\" in field:\n",
    "                        self.gap_ext = int(field.strip(\"XG:i\"))\n",
    "                    elif \"NM:i\" in field:\n",
    "                        self.edit_distance = int(field.strip(\"NM:i\"))\n",
    "                    elif \"YF:Z\" in field:\n",
    "                        self.why_filtered = int(field.strip(\"YF:Z\"))\n",
    "                    elif \"YT:Z\" in field:\n",
    "                        self.yt = field.strip(\"YT:Z\")\n",
    "                    elif \"MD:Z\" in field:\n",
    "                        self.mismatch_ref_bases = field.strip(\"MD:Z\")\n",
    "\n",
    "\n",
    "def parseCigarStr(cigarStr):\n",
    "    \"\"\"Parse thought a cigar string from the SAM file\n",
    "    and return a list for each cigar feature\"\"\"\n",
    "    lengthOfCigarFeature = \"\"\n",
    "    cigarList = []\n",
    "    for char in cigarStr:\n",
    "        if char in {\"S\", \"I\", \"M\", \"D\", \"X\", \"N\", \"H\", \"P\", \"=\"}:\n",
    "            cigarList.append((char, int(lengthOfCigarFeature)))\n",
    "            lengthOfCigarFeature = \"\"\n",
    "        else:\n",
    "            try:\n",
    "                lengthOfCigarFeature = lengthOfCigarFeature + char\n",
    "            except:\n",
    "                raise Exception(\"Unexpected char in cigar string: {}\".format(char))\n",
    "    return cigarList\n",
    "\n",
    "\n",
    "def extractFromCigar(feature, cigar):\n",
    "    # Exract a speficied feature from a cigar string\n",
    "    return sum([match[1] for match in parseCigarStr(cigar) if match[0] == feature])\n",
    "\n",
    "\n",
    "def isFileGzip(fileName):\n",
    "    \"\"\"\n",
    "    Inputs a file name and returns a boolean \n",
    "    of if the input file is gzipped\n",
    "    \"\"\"\n",
    "    if fileName[-3:] == \".gz\":\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def fastaOrFastq(fileName):\n",
    "    \"\"\"\n",
    "    Checks if the input file is in fasta or fastq format\n",
    "\n",
    "    args:\n",
    "    file_name(str): Name of the file to be checked\n",
    "\n",
    "    returns: A string \"fasta\" if the file is a fasta\n",
    "            file and \"fastq\" if if file is fastq\n",
    "    \"\"\"\n",
    "    if isFileGzip(fileName):\n",
    "        with gzip.open(fileName, \"rb\") as fn:\n",
    "            line = fn.readline()\n",
    "            if str(line)[2:].startswith(\"@\"):\n",
    "                return \"fastq\"\n",
    "            elif str(line)[2:].startswith(\">\"):\n",
    "                return \"fasta\"\n",
    "            else:\n",
    "                raise IOError\n",
    "    else:\n",
    "        with open(fileName) as fn:\n",
    "            line = fn.readline()\n",
    "            if line.startswith(\"@\"):\n",
    "                return \"fastq\"\n",
    "            elif line.startswith(\">\"):\n",
    "                return \"fasta\"\n",
    "            else:\n",
    "                raise IOError\n",
    "\n",
    "\n",
    "def decompose_sam_flag(flag):\n",
    "    \"\"\"\n",
    "    Decompose SAM flag into its component parts\n",
    "\n",
    "    Parameters:\n",
    "        flag(int): Sam flag\n",
    "\n",
    "    Returns:\n",
    "        (list): Elements of sam flag\n",
    "    \"\"\"\n",
    "    out_list = []\n",
    "    flag_list = [\"PAIRED\", \"PROPER_PAIR\", \"UNMAP\", \"MUNMAP\",\n",
    "                 \"REVERSE\", \"MREVERSE\", \"READ1\", \"READ2\",\n",
    "                 \"SECONDARY\", \"QCFAIL\", \"DUP\", \"SUPPLEMENTARY\"]\n",
    "\n",
    "    binary = str(f\"{flag:b}\"[::-1])\n",
    "    for i in range(len(binary)):\n",
    "        if binary[i] == \"1\":\n",
    "            out_list.append(flag_list[i])\n",
    "    return out_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "094751ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "align = paired_reads_with_bowtie2(\"/Users/kobiekirven/Desktop/scimsTest/GRCh38_latest_genomic.fna\", \n",
    "                     \"/Users/kobiekirven/Documents/GitHub/metagenomic-sex-determination/male1.fa\",\n",
    "                     \"/Users/kobiekirven/Documents/GitHub/metagenomic-sex-determination/male2.fa\", \n",
    "                    \"2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a77a1afe",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/70/5xrw6jjd1bg38ybt7xxf7nfh0000gn/T/ipykernel_16541/3592440905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0munmergedDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_unmerged_sam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbwa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/70/5xrw6jjd1bg38ybt7xxf7nfh0000gn/T/ipykernel_16541/1743716141.py\u001b[0m in \u001b[0;36mfilter_unmerged_sam\u001b[0;34m(unmerged_sam, diff_from_max_percent, min_match, num_deletions, mismatch_ratio)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mread_pair\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mtotal_num_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_matches\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_matches\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mtotal_num_mismatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_mismatches\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_mismatches\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m         \u001b[0mtotal_num_deletions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_deletions\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_deletions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mtotal_alignment_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alignment_score\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mread_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"alignment_score\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "unmergedDf = filter_unmerged_sam(bwa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c34782d",
   "metadata": {},
   "outputs": [],
   "source": [
    " counts = count_chrom(align, \"NC_000023.11\", \"NC_000024.10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a88e63fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.219, 0.002)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_stats(counts[0], counts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "889070fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104220, 29167)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8441a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
